{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Imports ######\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 7999 of 8000369 of 8000 569 of 8000 745 of 8000886 of 8000 1141 of 80001280 of 8000 1418 of 8000 1542 of 8000 1657 of 8000 1773 of 8000 1902 of 8000 2140 of 8000 of 8000 2593 of 8000 2713 of 8000 2816 of 8000 2924 of 8000 3118 of 8000 3229 of 80003327 of 8000 of 8000 3577 of 8000 3693 of 8000 3800 of 8000 3916 of 8000 4154 of 8000 4406 of 8000 4522 of 8000 4628 of 8000 4735 of 8000 4873 of 8000 5115 of 8000 5348 of 8000 5479 of 8000 of 8000 5679 of 8000 5873 of 8000 5960 of 8000 6103 of 8000 6203 of 80006297 of 8000 6398 of 8000 6485 of 8000 6664 of 8000 of 8000 6839 of 8000 6918 of 8000 7097 of 80007195 of 80007292 of 8000 7387 of 8000 7480 of 8000 7663 of 8000 7755 of 8000 7847 of 8000 7926 of 8000\n",
      "{'docID': 0}\n",
      "{'text': 'Showers continued throughout the week inthe Bahia cocoa zone, alleviating th\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes XML tags from the data\n",
    "def clean_data(data):\n",
    "    try:\n",
    "        tags = ET.fromstring(str(data))\n",
    "        return ET.tostring(tags, method='text').decode(\"utf-8\")\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "# Finds all different places and removes the XML tags from them\n",
    "def clean_places(places):\n",
    "    placelist = []\n",
    "    for i in places:\n",
    "        tags = ET.fromstring(str(i))\n",
    "        placelist.append(ET.tostring(tags, method='text').decode(\"utf-8\"))\n",
    "        \n",
    "    return placelist\n",
    "\n",
    "# Removes title, place and the XML tags from the text\n",
    "def clean_text(text):\n",
    "    for tag in text.findAll():\n",
    "        tag.replaceWith('')\n",
    "\n",
    "    return text.text.replace('\\n','')\n",
    "\n",
    "# Creates dictionary with all documents in the Reuters database, assigns doc_ID to all documents. \n",
    "def load_data():\n",
    "    data_dict = {}\n",
    "    data_id = 0\n",
    "    data_str = ''\n",
    "    for i in range(8):\n",
    "        data1 = open('reuters_data/reut2-00'+ str(i) +'.sgm')\n",
    "        soup = BeautifulSoup(data1,'lxml')\n",
    "        items = soup.findAll('reuters')\n",
    "        \n",
    "        for doc in items:\n",
    "            ID =     str({'docID':data_id})\n",
    "            data =   str({'date':clean_data(doc.date),\n",
    "                          'topic':clean_data(doc.topics), \n",
    "                          'place':clean_places(doc.places.findAll('d')), \n",
    "                          'people':clean_data(doc.people), \n",
    "                          'orgs':clean_data(doc.orgs), \n",
    "                          'exchanges':clean_data(doc.exchanges), \n",
    "                          'companies':clean_data(doc.companies), \n",
    "                          'title':clean_data(doc.title), \n",
    "                          'text':clean_text(doc.findAll('text')[0]) \n",
    "                      } )\n",
    "            data_str = data_str + '\\n' + ID + '\\n' + data\n",
    "            print('Document', data_id, 'of 8000', end='\\r')\n",
    "            data_id += 1\n",
    "    \n",
    "    with open('reuters.json', 'w') as f:\n",
    "        json.dump(data_str, f)\n",
    "    \n",
    "    return 'done'\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FILENAME, mode='w', encoding='utf-8') as feedsjson:\n",
    "    entry = {'name': args.name, 'url': args.url}\n",
    "    feeds.append(entry)\n",
    "    json.dump(feeds, feedsjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
