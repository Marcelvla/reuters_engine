{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes XML tags from the data\n",
    "def clean_data(data):\n",
    "    try:\n",
    "        tags = ET.fromstring(str(data))\n",
    "        return ET.tostring(tags, method='text').decode(\"utf-8\")\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "# Finds all different places and removes the XML tags from them\n",
    "def clean_places(places):\n",
    "    placelist = []\n",
    "    for i in places:\n",
    "        tags = ET.fromstring(str(i))\n",
    "        placelist.append(ET.tostring(tags, method='text').decode(\"utf-8\"))\n",
    "        \n",
    "    return placelist\n",
    "\n",
    "# Removes title, place and the XML tags from the text\n",
    "def clean_text(text):\n",
    "    for tag in text.findAll():\n",
    "        tag.replaceWith('')\n",
    "\n",
    "    return text.text.replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with one review:\n",
    "def makewordcloud(text,titel):\n",
    "\n",
    "    # Create and generate a word cloud image:\n",
    "    if len(text) > 10:\n",
    "        wordcloud = WordCloud().generate(text)\n",
    "\n",
    "        # Display the generated image:\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        formating=\".png\"\n",
    "        print(titel)\n",
    "        wordcloud.to_file(\"images/%d.png\" %titel)\n",
    "\n",
    "\n",
    "# text2=soup.findAll(\"text\")\n",
    "# print(len(text2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# items = soup.findAll('reuters')\n",
    "        \n",
    "# for doc in items:\n",
    "#     title=clean_data(doc.title), \n",
    "#     text=clean_text(doc.findAll('text')[0])\n",
    "\n",
    "\n",
    "def run(text):\n",
    "\n",
    "    for i in range(0,len(text)):\n",
    "        text=soup.findAll(\"text\")[i].renderContents()\n",
    "        titel=soup.findAll(\"title\")[i].renderContents()\n",
    "        text=str(text)\n",
    "        new=text[text.find('</dateline>'):]\n",
    "        new2=new.replace('</dateline>', '')\n",
    "        new3=new2.replace('\\\\n', '')\n",
    "        new4= ' '.join([word for word in new3.split() if word not in stopwords.words(\"english\")])\n",
    "        titel2=str(titel).replace(\"/\",\" \")\n",
    "        makewordcloud(new4,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reut2-000.sgm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8ad5e00167dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reut2-00'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.sgm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reuters'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reut2-000.sgm'"
     ]
    }
   ],
   "source": [
    " for i in range(8):\n",
    "        data1 = open('reuters_data/reut2-00'+ str(i) +'.sgm')\n",
    "        soup = BeautifulSoup(data1,'lxml')\n",
    "        items = soup.findAll('reuters')\n",
    "        all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ##### Imports ######\n",
    "# from bs4 import BeautifulSoup\n",
    "# import xml.etree.ElementTree as ET\n",
    "# # Removes XML tags from the data\n",
    "# def clean_data(data):\n",
    "#     try:\n",
    "#         tags = ET.fromstring(str(data))\n",
    "#         return ET.tostring(tags, method='text').decode(\"utf-8\")\n",
    "#     except:\n",
    "#         return data\n",
    "\n",
    "# # Finds all different places and removes the XML tags from them\n",
    "# def clean_places(places):\n",
    "#     placelist = []\n",
    "#     for i in places:\n",
    "#         tags = ET.fromstring(str(i))\n",
    "#         placelist.append(ET.tostring(tags, method='text').decode(\"utf-8\"))\n",
    "        \n",
    "#     return placelist\n",
    "\n",
    "# # Removes title, place and the XML tags from the text\n",
    "# def clean_text(text):\n",
    "#     for tag in text.findAll():\n",
    "#         tag.replaceWith('')\n",
    "\n",
    "#     return text.text.replace('\\n','')\n",
    "\n",
    "# # Creates dictionary with all documents in the Reuters database, assigns doc_ID to all documents. \n",
    "# def load_data():\n",
    "#     data_dict = {}\n",
    "#     data_id = 0\n",
    "#     for i in range(8):\n",
    "#         data1 = open('reut2-00'+ str(i) +'.sgm')\n",
    "#         soup = BeautifulSoup(data1,'lxml')\n",
    "#         items = soup.findAll('reuters')\n",
    "        \n",
    "#         for doc in items:\n",
    "#             data_dict[data_id] = {'date':clean_data(doc.date),\n",
    "#                                   'topic':clean_data(doc.topics), \n",
    "#                                   'place':clean_places(doc.places.findAll('d')), \n",
    "#                                   'people':clean_data(doc.people), \n",
    "#                                   'orgs':clean_data(doc.orgs), \n",
    "#                                   'exchanges':clean_data(doc.exchanges), \n",
    "#                                   'companies':clean_data(doc.companies), \n",
    "#                                   'title':clean_data(doc.title), \n",
    "#                                   'text':clean_text(doc.findAll('text')[0]) }\n",
    "#             data_id += 1 \n",
    "            \n",
    "#     return data_dict\n",
    "        \n",
    "# data = load_data()\n",
    "\n",
    "\n",
    "# for i in range(0,len(data)):\n",
    "#      makewordcloud2(data[i][\"text\"],data[i][\"title\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makewordcloud2(text,title):\n",
    "\n",
    "    # Create and generate a word cloud image:\n",
    "    if len(text) > 10:\n",
    "        wordcloud = WordCloud().generate(text)\n",
    "\n",
    "        # Display the generated image:\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        titel=title.replace(\" \",\"\")\n",
    "        formating=\".png\"\n",
    "        combine=titel+formating\n",
    "        print(combine)\n",
    "        wordcloud.to_file(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
